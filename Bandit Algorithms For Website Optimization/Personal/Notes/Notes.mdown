# Definitions (all in the context of slot machines in a casino)

1. Reward - A quantitative measure of success. Requires two things (A) there is a clear quantitative scale and (B) it’s better to have more reward than less reward.

2. Arm - What options are available to us? 

3. Bandit - A bandit a complicated slot machine, that can one have or many arms

4. Play/Trial - Each chance you have to pull on an arm will be called a play or, more often, a trial. 

5. Horizon - The number of trials left

6. Exploitation - Plays the arm with the highest estimated value based on previous plays.

7. Exploration - Plays any arm that does not have the highest estimated value based on previous plays. In other words, exploration occurs whenever exploitation does not.

8. Explore/Exploit Dilemma - The observation that any learning system must strike a compromise between its impulse to explore and its impulse to exploit. 

9. Annealing - An algorithm for solving the Multi-armed Bandit Problem anneals if it explores less over time.

10. Temperature - A parameter that can be adjusted to increase the amount of exploration in the Soft-max algorithm for solving the Multi-armed Bandit Problem. If you decrease the temperature parameter over time, this causes the algorithm to anneal.

11. Streaming Algorithms - An algorithm is a streaming algorithm if it can process data one piece at a time. This is the opposite of batch processing algorithms that need access to all of the data in order to do anything with it.

12. On-line Learning - An algorithm is an on-line learning algorithm if it can not only process data one piece at a time, but can also provide provisional results of its analysis after each piece of data is seen.

13. Active Learning
An algorithm is an active learning algorithm if it can decide which pieces of data it wants to see next in order to learn most effectively. Most traditional machine learn‐ ing algorithm are not active: they passively accept the data we feed them and do not tell us what data we should collect next.

14. Bernoulli - A Bernoulli system outputs a 1 with probability p and a 0 with probability 1 – p. (heads or tails is a good example)

# Chapter 1 - Two Characters: Exploration and Exploitation

- Two big takeaways from the Scientist (who loves to experiment) vs. the business man (who was skeptical and risk averse)
- Needed a simple way to balance the need to 
	(1) learn new things
	(2) profit from old things that she’d already learned."

- Weakness of A/B Testing (Example being given is about changing a logo design color)
	- Fixed Horizons
	- Doesn't address this
		- If you do too much exploration, you lose money. And if you do too much exploitation, you stagnate and miss out on new opportunities
	- For example, a trial period of A/B testing followed by sticking strictly to the best design afterwards only makes sense if there is a definite best design that consistently works (Example: Changing the logo color to things that work during only Christmas, and then seeing a drop in sales every other month)
	-  If you run an experiment that stretches across both times of year, you’ll see no average effect for your two color schemes — even though there’s a huge effect in each of the seasons if you had examined them separately. 

# Chapter 2 - Why Use Multiarmed Bandit Algorithms? 
- Success can be measured with Traffice (increase), conversion, sales, CTRs (Click Through Rates)

Standard A/B Testing
- A short period of pure exploration, in which you assign equal numbers of users to Groups A and B.
- A long period of pure exploitation, in which you send all of your users to the more successful version of your site and never come back to the option that seemed to be inferior.

Why is this a bad strategy?
- No smooth transition from exploration to exploitation
- During exploratory phase, you're wasting a lot of resources 

- All good bandit algorithms will eventually converge.

# Chapter 3 - The epsilon-Greedy Algorithm
- Greedy algorithm - an algorithm that always takes whatever action seems best at the present moment, even when that decision might lead to bad long term consequences

- INSERT IMAGE HERE (Figure 3-1. The epsilon-Greedy arm selection process)

- Oscillates between (A) exploiting the best option that it currently knows about and (B) exploring at random among all of the options available to it

- What's a Bandit Problem? - Defined by the following features

1. Facing a complicated slot machine, with a set of N arms
2. When pulled, arms will give rewards (but unreliable at different payout schedules)
3. We don't know the reward rates for each of the arms. Forced to pull arms
4. Whatever arm we pull, we will gain information about that arm, but miss out on other arms.
5. Every time we experiment with an arm that isn’t the best arm, we lose reward be‐ cause we could, at least in principle, have pulled on a better arm.

- If you have a multi-armed bandit problem, statistics would naturally lead you to calculate the highest reward average for each arm. But reason 4 and 5 above is what makes it more complicated/realistic

- If we were to stick to a 50/50 split between two logos, the experiment can be ran as such

        algo = EpsilonGreedy(1.0, [])
            initialize(algo, 2)
            
- Or if we were to not explore at all and just explot, we could write:
         algo.epsilon = 0.0

**Weaknesses of Epsilon Greedy**
- As you become more certain of the best option, the tendency to explore the worse design a full 5% of the time will become more wasteful (if epsilon is set to 10%, you will be exploring the "bad" scenario 5% of the time). This is called over-exploring
- You’ll choose options that you don’t know much about far more rarely than you’d like to because you only try new options 10% of the time.

# Chapter 4 - Debugging Bandit Algorithms
